{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Nf927wR4OdskN9km_0vkQpG2mmrAKSbI",
      "authorship_tag": "ABX9TyMLbds5WHjO0dfQhC1jGUHu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Savith-02/notebooks/blob/main/nn_intent_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9jFXoOHZYAG",
        "outputId": "d5f5ccc5-b2fe-4b92-8bcf-a48257d8f89a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Greet', 'Hi'), ('Greet', 'Hello'), ('Greet', 'Hey there'), ('Greet', 'Good morning'), ('Greet', 'Howdy'), ('Greet', 'Hi there'), ('Greet', 'Hey'), ('Greet', 'Good afternoon'), ('Greet', 'Hello there'), ('Greet', 'Hi how can I help you?')]\n"
          ]
        }
      ],
      "source": [
        "def read_dataset(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            intent, sentence = line.strip().split(',')\n",
        "            data.append((intent.strip(), sentence.strip()))\n",
        "    return data\n",
        "\n",
        "file_path = \"drive/MyDrive/Code/rawData/data_small.txt\"\n",
        "dataset = read_dataset(file_path)[1:]\n",
        "print(dataset[:10])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split = round(len(dataset) * 60 / 100)\n",
        "# training_data = dataset[:split]\n",
        "# cv_data = dataset[split:]\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "training_data, test_data = train_test_split(dataset, test_size=0.4, random_state=42)\n"
      ],
      "metadata": {
        "id": "Ik4zCVRLZu6M"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intent_counts = {}\n",
        "for intent, _ in dataset:\n",
        "    if intent not in intent_counts:\n",
        "        intent_counts[intent] = 0\n",
        "    intent_counts[intent] += 1\n",
        "print(intent_counts)\n",
        "print(f\"Number of examples in dataset: {len(dataset)}\")\n",
        "print(f\"Number of examples in training dataset: {len(training_data)}\")\n",
        "print(f\"Number of examples in cv dataset: {len(test_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fRpvOzYZyNr",
        "outputId": "829e5e33-18c0-4990-8f48-e7fcbb1844b7"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Greet': 53, 'Farewell': 66, 'Inquiry': 50, 'Feedback': 50, 'Complaint': 38, 'Request': 50, 'Navigation': 58}\n",
            "Number of examples in dataset: 365\n",
            "Number of examples in training dataset: 219\n",
            "Number of examples in cv dataset: 146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_sentences = set(sentence for intent, sentence in dataset)\n",
        "print(f\"All unique sentences count: {len(all_sentences)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "al9Wbj_PdWNH",
        "outputId": "163955cc-2d65-431d-8d27-a198ea42ed38"
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All unique sentences count: 355\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicates\n",
        "# from collections import Counter\n",
        "# word_counts = Counter(sentence for intent, sentence in dataset)\n",
        "# word_counts"
      ],
      "metadata": {
        "id": "F71YYZb4dUKP"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import chain\n",
        "\n",
        "all_words = set(chain(*[sentence.split(\" \") for intent, sentence in dataset]))\n",
        "print(f\"All unique sentences count: {len(all_words)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3xH2Wv6fxBG",
        "outputId": "1bd7aed3-ed93-46a2-f256-54e120da7426"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All unique sentences count: 768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "9PE2p0FYZ1Ps"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index = {char: i for i, char in enumerate(all_words)}\n",
        "print(f\"No of words: {len(all_words)}\")\n",
        "print(word_to_index.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpxu-BcwZ_1s",
        "outputId": "5c392648-4d71-47f0-f9ac-70faac1101f9"
      },
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No of words: 768\n",
            "dict_items([('way', 0), ('share', 1), ('until', 2), ('color?', 3), ('doing', 4), ('assist', 5), ('solution', 6), ('excellent', 7), ('pm.', 8), ('by', 9), ('lot', 10), ('reliable', 11), ('seamless', 12), ('soon!', 13), ('Italian', 14), ('change', 15), ('navigate', 16), ('and', 17), ('many', 18), ('explanations.', 19), ('advanced', 20), ('easy!', 21), ('incorrect.', 22), ('understand', 23), (\"mornin'\", 24), ('bookmark', 25), ('one', 26), ('leaky', 27), ('up?', 28), ('make', 29), ('effort', 30), ('impairments.', 31), ('year?', 32), ('hiking', 33), ('hotel', 34), ('works?', 35), ('pricing', 36), ('fit', 37), ('issue?', 38), ('items', 39), ('help', 40), ('message', 41), ('easy', 42), ('tutorial', 43), ('Paris', 44), ('tight!', 45), ('here', 46), ('shall', 47), ('next', 48), ('awesome', 49), ('explain', 50), ('mailing', 51), ('Having', 52), ('certain', 53), ('Goodnight!', 54), (\"farmers'\", 55), ('summarize', 56), ('responsive', 57), ('new?', 58), ('query?', 59), ('all!', 60), ('upload', 61), ('issues.', 62), ('section', 63), ('pharmacy?', 64), ('hotel?', 65), ('job', 66), ('below', 67), ('adventure', 68), ('shopping', 69), ('There', 70), ('problem.', 71), ('everything?', 72), ('trouble', 73), ('My', 74), ('prompt', 75), ('reflect', 76), ('allergies?', 77), ('Take', 78), ('tourist', 79), ('show', 80), ('section.', 81), ('dissatisfied', 82), ('provide', 83), ('would', 84), ('poor.', 85), (\"How's\", 86), ('bad', 87), ('later?', 88), ('the', 89), ('send', 90), ('support?', 91), ('Goodnight', 92), (\"You've\", 93), ('outcome.', 94), ('break', 95), ('gas', 96), ('Morning', 97), ('a', 98), ('Welcome', 99), ('temperature', 100), ('available?', 101), ('different', 102), ('friend', 103), ('Glad', 104), ('problem?', 105), ('cookies?', 106), ('main', 107), ('these', 108), ('disappointing.', 109), ('tomorrow', 110), ('Your', 111), ('else', 112), ('Restart', 113), ('like', 114), ('brightly', 115), ('favorite', 116), ('Empire.', 117), ('stories', 118), ('stay', 119), ('Howdy', 120), ('euros?', 121), ('demographics.', 122), ('fancy', 123), ('effective.', 124), ('work', 125), ('meaning', 126), ('meet', 127), ('every', 128), ('matter.', 129), ('long!', 130), ('spot', 131), ('2', 132), ('pizza', 133), ('ya!', 134), ('recipe.', 135), ('thanks.', 136), ('downtown?', 137), ('user-friendly.', 138), ('am', 139), ('gift', 140), ('7', 141), (\"It's\", 142), ('system?', 143), ('touch', 144), ('class?', 145), ('through', 146), ('banana?', 147), ('public', 148), ('flights.', 149), ('unsatisfactory.', 150), ('unacceptable.', 151), ('efficient.', 152), ('list', 153), ('defective.', 154), ('reviews', 155), ('Hello', 156), ('taxes', 157), ('again', 158), ('used.', 159), ('route', 160), ('weekend.', 161), ('account', 162), ('data', 163), ('coffee?', 164), ('everyone', 165), (\"how's\", 166), (\"I'd\", 167), ('this.', 168), ('Logout', 169), ('paths', 170), ('issue.', 171), ('nice', 172), ('feature.', 173), ('significant', 174), ('get', 175), ('up.', 176), ('are', 177), ('process?', 178), (\"what's\", 179), ('do', 180), ('some', 181), ('learn', 182), ('options', 183), ('aboard!', 184), ('history?', 185), ('pleasure', 186), ('news?', 187), ('work.', 188), ('difficult', 189), ('upgrade.', 190), ('window', 191), ('there', 192), ('this', 193), ('Catch', 194), ('Top', 195), ('with', 196), ('Go', 197), ('Show', 198), ('traffic', 199), ('time', 200), ('side', 201), ('population', 202), ('clean', 203), ('app', 204), ('further?', 205), ('nearest', 206), ('biased', 207), ('everyone!', 208), ('tab', 209), ('fix', 210), ('stop?', 211), ('Have', 212), ('book?', 213), ('Hey', 214), ('person', 215), ('our', 216), ('needed.', 217), ('intuitive.', 218), ('meeting', 219), ('accessible', 220), ('at', 221), ('back', 222), ('that', 223), ('How', 224), ('but', 225), ('for', 226), ('getting', 227), ('actually', 228), ('slight', 229), ('afternoon', 230), ('potential', 231), ('in', 232), ('fantastic', 233), ('previous', 234), ('particularly', 235), ('friend!', 236), ('service', 237), ('product.', 238), ('ways.', 239), ('my', 240), ('pleased', 241), ('balance?', 242), ('...', 243), ('known', 244), ('customize', 245), ('you', 246), ('Close', 247), ('job.', 248), ('has', 249), ('tools', 250), ('room', 251), ('developing', 252), ('You', 253), ('resource', 254), ('error', 255), ('movie', 256), ('explore', 257), ('talk', 258), ('me', 259), ('edit', 260), ('terminology', 261), ('exceeded', 262), ('purchasing', 263), ('tool.', 264), ('definitely', 265), ('into', 266), ('Can', 267), ('menu', 268), ('privacy', 269), ('product', 270), ('no', 271), ('really', 272), ('It', 273), (\"I'm\", 274), ('long', 275), ('pictures', 276), ('Until', 277), ('concerned', 278), ('went', 279), ('calendar', 280), ('keep', 281), ('article', 282), ('understanding', 283), ('design', 284), ('Bye', 285), ('continues', 286), ('file', 287), (\"o'\", 288), ('accuracy', 289), ('document?', 290), ('organic', 291), ('process', 292), ('smiling', 293), ('calories', 294), ('City?', 295), ('interface', 296), ('develop', 297), ('from', 298), ('point.', 299), ('informative', 300), ('which', 301), ('Hi', 302), ('reservation?', 303), ('needs.', 304), ('service?', 305), ('Excellent', 306), ('it', 307), ('anything', 308), ('bank', 309), ('functionality', 310), ('expectations', 311), ('station?', 312), ('late.', 313), ('results', 314), ('description', 315), ('very', 316), ('me?', 317), ('as', 318), ('rocking!', 319), ('we', 320), ('save', 321), ('rendezvous', 322), ('responsive.', 323), ('much', 324), ('unhappy', 325), ('this?', 326), ('The', 327), ('visualization', 328), ('morning', 329), ('clarify', 330), ('play', 331), ('center?', 332), ('advertised', 333), ('well', 334), ('its', 335), ('who', 336), ('assistance', 337), ('app.', 338), ('to', 339), ('warm', 340), ('Save', 341), ('local', 342), (\"Let's\", 343), ('contact', 344), ('package?', 345), ('how', 346), ('stargazing?', 347), ('additional', 348), ('game', 349), ('schedule', 350), ('clear', 351), ('order', 352), ('place', 353), ('outdated', 354), ('made', 355), ('good', 356), ('excellent.', 357), ('menu.', 358), ('having', 359), ('playing', 360), ('sleep', 361), ('feature', 362), ('time!', 363), ('watch', 364), ('progress', 365), ('assistance?', 366), ('book', 367), ('voice', 368), ('assistance.', 369), ('or', 370), ('positive.', 371), ('support', 372), ('visual', 373), ('seems', 374), ('later', 375), ('provided', 376), ('night?', 377), ('was', 378), ('annoying.', 379), ('you.', 380), ('sunset?', 381), ('accurately', 382), ('sort', 383), ('See', 384), ('ingredients', 385), ('out', 386), ('response.', 387), ('Is', 388), ('content', 389), ('Be', 390), ('week?', 391), ('faucet?', 392), ('customer', 393), ('pet-friendly', 394), ('advice', 395), ('happening?', 396), ('page.', 397), ('thought', 398), ('whenever', 399), ('disabilities.', 400), ('Where', 401), ('Spanish?', 402), ('online?', 403), ('mistake', 404), ('exactly', 405), ('dog-friendly', 406), ('performance', 407), ('before', 408), ('more', 409), (\"What's\", 410), ('morning.', 411), ('Open', 412), ('track', 413), ('guitar?', 414), ('Sort', 415), ('access', 416), ('what', 417), ('artist?', 418), ('safe', 419), ('move', 420), ('if', 421), ('delivery', 422), ('service.', 423), ('team', 424), ('happy', 425), (\"it's\", 426), ('Python?', 427), ('specific', 428), ('groceries?', 429), ('Overall', 430), ('awesome!', 431), ('best', 432), ('word?', 433), ('not', 434), ('plumber?', 435), ('events', 436), ('crashing', 437), ('page?', 438), ('take', 439), ('go', 440), ('could', 441), (\"I've\", 442), ('navigate.', 443), ('company.', 444), ('technology.', 445), ('users', 446), ('thriving', 447), ('capital', 448), ('transportation', 449), ('up!', 450), ('today?', 451), ('ya', 452), ('information', 453), ('news', 454), ('frustrated', 455), ('understand.', 456), ('far', 457), ('other', 458), ('policies', 459), ('give', 460), ('serve', 461), ('impressed', 462), ('sushi-making', 463), ('destinations.', 464), ('recipe', 465), (\"mother's\", 466), ('fast', 467), ('pick', 468), ('buy', 469), ('professional', 470), ('song', 471), ('find', 472), ('task?', 473), ('wait', 474), ('frequently.', 475), ('category', 476), ('impressive.', 477), ('it!', 478), ('return', 479), ('Stay', 480), ('keeps', 481), ('facts', 482), ('tomorrow!', 483), ('see', 484), ('par.', 485), ('powerful', 486), ('chocolate', 487), ('today.', 488), ('love', 489), ('come', 490), ('system', 491), ('music', 492), ('your', 493), ('be', 494), ('chip', 495), ('bill.', 496), ('time?', 497), ('excited', 498), ('to?', 499), ('price?', 500), ('movies', 501), ('doing?', 502), ('requesting', 503), ('disappointed', 504), ('exchange', 505), ('tell', 506), ('evening', 507), ('resolve', 508), (\"There's\", 509), ('flip', 510), ('file.', 511), ('dog', 512), ('forecast', 513), ('about', 514), ('Thanks', 515), ('support.', 516), ('page', 517), ('is', 518), ('city', 519), ('friend?', 520), ('going?', 521), ('library', 522), ('details.', 523), ('appreciated.', 524), ('add', 525), ('expensive', 526), ('helpful', 527), ('print', 528), ('level', 529), ('cart?', 530), ('quality', 531), ('unclear', 532), (\"didn't\", 533), ('Log', 534), ('can', 535), ('Roman', 536), ('using', 537), ('Please', 538), ('later!', 539), ('log', 540), ('specializes', 541), ('valuable', 542), ('delivery?', 543), ('okay?', 544), ('detailed', 545), ('brand', 546), ('class!', 547), ('website', 548), ('release?', 549), ('features', 550), ('A', 551), ('strong', 552), ('bus', 553), ('results.', 554), ('ride-sharing', 555), ('thank', 556), ('received.', 557), ('situation?', 558), ('set', 559), ('curve', 560), ('shining', 561), ('language', 562), ('What', 563), ('beginning', 564), ('innovative', 565), ('tutorials', 566), ('connect', 567), ('\"ambiguous.\"', 568), ('information?', 569), ('link', 570), ('too', 571), ('welcome', 572), ('tickets?', 573), ('going', 574), ('invaluable', 575), ('doctor', 576), ('problem', 577), ('of', 578), ('appreciate', 579), ('now', 580), ('reset', 581), ('beyond.', 582), ('an', 583), ('please', 584), ('had', 585), ('recent', 586), ('latest', 587), ('live', 588), ('lack', 589), ('learning', 590), ('regret', 591), ('Good', 592), ('user', 593), ('definition', 594), ('day!', 595), ('work!', 596), ('Goodbye', 597), ('unhelpful.', 598), ('little', 599), ('city?', 600), ('weather', 601), ('won', 602), ('looking', 603), ('up', 604), ('seeking', 605), ('rate', 606), ('text', 607), ('incredibly', 608), ('response', 609), ('care', 610), ('Adios', 611), ('terrible.', 612), ('Keep', 613), ('Could', 614), ('breeds?', 615), ('times', 616), ('model', 617), ('package', 618), ('program', 619), ('rent', 620), ('kind', 621), ('near', 622), ('were', 623), ('answer?', 624), ('search', 625), ('experience.', 626), ('Great', 627), ('exceptional.', 628), ('great!', 629), ('translate', 630), ('audio', 631), ('unexpectedly.', 632), ('date', 633), ('feedback', 634), ('topic', 635), ('child', 636), ('new', 637), ('restaurant?', 638), ('upcoming', 639), ('confident', 640), ('airport?', 641), ('experience', 642), ('use', 643), ('calendar.', 644), ('refine', 645), ('bye', 646), ('cross', 647), ('overall', 648), ('Who', 649), ('check', 650), ('settings', 651), ('customization', 652), ('enroll', 653), ('it.', 654), ('recommend', 655), ('historical', 656), ('account.', 657), ('Filter', 658), ('Move', 659), ('buddy', 660), ('soon', 661), ('York', 662), ('table', 663), ('impact.', 664), ('theaters.', 665), ('fishing?', 666), ('terms', 667), ('topic?', 668), ('did', 669), ('on', 670), ('car?', 671), ('have', 672), ('item', 673), ('will', 674), ('detail', 675), ('capabilities.', 676), ('I', 677), ('alarm', 678), ('format?', 679), ('criteria', 680), ('food?', 681), ('password?', 682), ('need', 683), ('town?', 684), ('drive?', 685), ('available.', 686), ('card?', 687), ('future.', 688), ('asset', 689), ('related', 690), ('park?', 691), ('Thank', 692), ('browsing', 693), ('great', 694), ('been', 695), ('market?', 696), ('you!', 697), ('day', 698), ('apply', 699), ('attractions?', 700), ('helpful.', 701), ('convenient.', 702), ('device?', 703), ('tonight?', 704), ('yourself', 705), ('last', 706), ('call', 707), ('fastest', 708), ('restrictive.', 709), ('options.', 710), ('airport.', 711), ('expected.', 712), ('been?', 713), ('try', 714), ('results?', 715), ('top', 716), ('example', 717), ('cup', 718), ('average', 719), ('expectations.', 720), ('window.', 721), ('France?', 722), ('guide', 723), ('you?', 724), ('Exit', 725), ('closest', 726), ('above', 727), ('further', 728), ('filter', 729), ('now!', 730), ('quite', 731), ('So', 732), ('feels', 733), ('New', 734), ('map', 735), ('started', 736), ('birthday?', 737), ('trail?', 738), ('travel', 739), ('not-too-distant', 740), ('help.', 741), ('fixing', 742), ('video', 743), ('commands?', 744), ('others.', 745), ('system.', 746), ('word', 747), ('satisfied', 748), ('then', 749), ('layout', 750), ('Print', 751), ('Hiya', 752), ('towards', 753), ('school?', 754), ('restaurants', 755), ('integration', 756), ('This', 757), ('Skip', 758), ('scenic', 759), ('step', 760), ('current', 761), ('future', 762), ('order.', 763), ('found', 764), ('address?', 765), ('we?', 766), ('makes', 767)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_intents = set(intent for intent, _ in dataset[1:])\n",
        "intent_to_index = {intent: i for i, intent in enumerate(all_intents)}\n",
        "\n",
        "print(f\"No of intents {len(all_intents)}\")\n",
        "print(intent_to_index.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbmsFe3IaQls",
        "outputId": "997243cc-7d3b-4a24-eb18-adde3a5f3503"
      },
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No of intents 7\n",
            "dict_items([('Feedback', 0), ('Greet', 1), ('Complaint', 2), ('Inquiry', 3), ('Navigation', 4), ('Request', 5), ('Farewell', 6)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert sentences to tensors\n",
        "def sentence_to_tensor(sentence):\n",
        "    # print(sentence)\n",
        "    tensor = torch.zeros(len(all_words)).to(device)\n",
        "    for i, word in enumerate(sentence.split(\" \")):\n",
        "        tensor[word_to_index[word]] = 1\n",
        "        # print(word)\n",
        "    # print(tensor)\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "CW4XiH4Rj4Ee"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def tensor_to_sentence(tensor, all_words, word_to_index):\n",
        "#     sentence = []\n",
        "#     for i in range(len(tensor)):\n",
        "#         if tensor[i] == 1:\n",
        "#             word = list(word_to_index.keys())[list(word_to_index.values()).index(i)]\n",
        "#             sentence.append(word)\n",
        "#     return \" \".join(sentence)\n"
      ],
      "metadata": {
        "id": "3hKDkeKcu7_G"
      },
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataset, feature_transform=None, label_transform=None):\n",
        "        self.data = dataset\n",
        "        self.feature_transform = feature_transform\n",
        "        self.label_transform = label_transform\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        return label_transform[sample[0]], feature_transform(sample[1])\n",
        "            # sample[1] = self.transform(sample[1])\n"
      ],
      "metadata": {
        "id": "Q5l0VCoBlcgm"
      },
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_transform = transforms.Compose([\n",
        "    sentence_to_tensor,\n",
        "])\n",
        "label_transform = intent_to_index"
      ],
      "metadata": {
        "id": "xtcBttw71sF4"
      },
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the datasets\n",
        "train_dataset = CustomDataset(training_data[:192], feature_transform=feature_transform, label_transform=label_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "# test_dataset = CustomDataset(test_data, test_labels)"
      ],
      "metadata": {
        "id": "RnEfoGCamxIW"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access individual samples from the dataset\n",
        "# for i in range(len(train_dataset[5:10])):\n",
        "#     sample = train_dataset[i]\n",
        "    # print(sample)"
      ],
      "metadata": {
        "id": "DxJ_7NgcoMgr"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch, (intent, sentence) in enumerate(train_loader):\n",
        "    if batch == 15:\n",
        "      break\n",
        "    print(sentence.shape)\n",
        "    print(intent.shape)\n",
        "    print(intent)\n",
        "\n",
        "    # data contains the input samples for the current batch\n",
        "    # target contains the corresponding labels for the current batch\n",
        "\n",
        "    # You can print the shapes of data and target to verify they are as expected\n",
        "    # print(sentence)\n",
        "    # print(len(sentence))\n",
        "    print(\"-----------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLE7vFqkocS-",
        "outputId": "33c24024-aaa8-48d7-ad11-256e6ae0531c"
      },
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 768])\n",
            "torch.Size([32])\n",
            "tensor([1, 1, 6, 4, 2, 4, 1, 0, 4, 4, 1, 3, 4, 3, 4, 3, 6, 4, 4, 0, 0, 1, 0, 3,\n",
            "        0, 6, 1, 1, 6, 3, 5, 1])\n",
            "-----------\n",
            "torch.Size([32, 768])\n",
            "torch.Size([32])\n",
            "tensor([1, 4, 5, 4, 2, 6, 2, 1, 4, 5, 1, 3, 4, 3, 3, 5, 5, 3, 0, 6, 5, 2, 3, 6,\n",
            "        4, 5, 6, 5, 3, 3, 4, 4])\n",
            "-----------\n",
            "torch.Size([32, 768])\n",
            "torch.Size([32])\n",
            "tensor([5, 2, 6, 0, 5, 6, 5, 1, 3, 5, 0, 0, 2, 5, 3, 2, 4, 6, 3, 5, 6, 4, 1, 5,\n",
            "        2, 0, 1, 6, 4, 1, 4, 2])\n",
            "-----------\n",
            "torch.Size([32, 768])\n",
            "torch.Size([32])\n",
            "tensor([4, 0, 3, 5, 4, 5, 3, 3, 6, 5, 6, 1, 2, 5, 0, 3, 6, 1, 0, 6, 6, 0, 2, 1,\n",
            "        4, 2, 5, 0, 3, 3, 1, 0])\n",
            "-----------\n",
            "torch.Size([32, 768])\n",
            "torch.Size([32])\n",
            "tensor([1, 3, 6, 2, 5, 0, 2, 0, 1, 3, 1, 4, 0, 0, 0, 3, 5, 3, 0, 6, 0, 5, 5, 4,\n",
            "        2, 6, 3, 6, 3, 2, 1, 1])\n",
            "-----------\n",
            "torch.Size([32, 768])\n",
            "torch.Size([32])\n",
            "tensor([3, 0, 5, 2, 6, 2, 6, 5, 0, 1, 4, 0, 5, 6, 0, 5, 2, 5, 6, 4, 2, 1, 0, 1,\n",
            "        6, 6, 1, 6, 0, 6, 0, 4])\n",
            "-----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(nn.Module):\n",
        "    def __init__(self, word_count, intent_class_count, hidden_units):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(word_count, hidden_units)\n",
        "        self.fc2 = nn.Linear(hidden_units, intent_class_count)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "BlCQAXnsHa_w"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(len(all_words), len(all_intents), 128)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "UJpcmN2rKH_x"
      },
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model, dataloader, reg_rate, optimizer, loss_fn):\n",
        "\n",
        "    model.train()\n",
        "    train_loss, train_acc = 0, 0\n",
        "\n",
        "    for batch, (intent, sentence) in enumerate(train_loader):\n",
        "\n",
        "        # intent_tensor = intent_to_tensor(intent)\n",
        "        train_pred_logits = model(sentence)\n",
        "        loss = loss_fn(train_pred_logits, intent)\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        loss = loss_fn(train_pred_logits, intent)\n",
        "\n",
        "        train_pred_class = torch.argmax(torch.softmax(train_pred_logits, dim=1), dim=1)\n",
        "        train_acc += (train_pred_class == intent).sum().item() / len(train_pred_class)\n",
        "\n",
        "    # Adjust metrics to get average loss and accuracy per batch\n",
        "    train_loss = train_loss / len(dataloader)\n",
        "    train_acc = train_acc / len(dataloader)\n",
        "    return train_loss, train_acc"
      ],
      "metadata": {
        "id": "7ZesrEimLuja"
      },
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(model, dataloader, reg_rate, loss_fn):\n",
        "\n",
        "    model.train()\n",
        "    train_loss, train_acc = 0, 0\n",
        "\n",
        "    for batch, (intent, sentence) in enumerate(train_loader):\n",
        "\n",
        "        # intent_tensor = intent_to_tensor(intent)\n",
        "        train_pred_logits = model(sentence)\n",
        "        loss = loss_fn(train_pred_logits, intent)\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        loss = loss_fn(train_pred_logits, intent)\n",
        "\n",
        "        train_pred_class = torch.argmax(torch.softmax(train_pred_logits, dim=1), dim=1)\n",
        "        train_acc += (train_pred_class == intent).sum().item() / len(train_pred_class)\n",
        "\n",
        "    # Adjust metrics to get average loss and accuracy per batch\n",
        "    train_loss = train_loss / len(dataloader)\n",
        "    train_acc = train_acc / len(dataloader)\n",
        "    return train_loss, train_acc"
      ],
      "metadata": {
        "id": "zZ-_DhdGPBkT"
      },
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, test_loader, hidden_size, learning_rate, reg_rate, epochs, optimizer, loss_function):\n",
        "\n",
        "    # Initialize the model\n",
        "    # model = model(num_chars, hidden_size, num_intents)\n",
        "    model.to(device)\n",
        "\n",
        "    loss_function = loss_function()\n",
        "    optimizer = optimizer(model.parameters(), lr=learning_rate)\n",
        "    # scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "\n",
        "    # Training loop with loss plotting\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    training_dataset_len = len(training_data)\n",
        "    test_dataset_len = len(test_data)\n",
        "\n",
        "    for batch, (intent, sentence) in enumerate(train_loader):\n",
        "\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for intent, sentence in training_data:\n",
        "            model.zero_grad()\n",
        "            # intent_tensor = intent_to_tensor(intent)\n",
        "            output = model(sentence)\n",
        "            loss = loss_function(output, intent)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # scheduler.step()\n",
        "        # train_losses.append(total_loss / training_dataset_len)\n",
        "        train_losses.append(total_loss)\n",
        "        # if (epoch + 1) % 10 == 0:\n",
        "        #     print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss}\")\n",
        "\n",
        "        model.eval()\n",
        "        total_test_loss = 0\n",
        "\n",
        "        with torch.inference_mode():\n",
        "          for batch, (intent, sentence) in enumerate(test_loader):\n",
        "              # intent = intent_to_tensor(intent)\n",
        "              output = model(sentence)\n",
        "              loss = loss_function(output, intent)\n",
        "              total_test_loss += loss.item()\n",
        "\n",
        "          test_losses.append(total_test_loss)\n",
        "\n",
        "    return train_losses, test_losses, model"
      ],
      "metadata": {
        "id": "QyVHFUeesWfM"
      },
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2VQ2mNmxPoGq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}